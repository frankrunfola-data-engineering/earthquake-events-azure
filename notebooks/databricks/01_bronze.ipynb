{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610eadf8",
   "metadata": {},
   "source": [
    "# Bronze — USGS Earthquake API → Raw JSON\n",
    "\n",
    "Fetch GeoJSON from the USGS API for a date range (typically daily), then write raw data to ADLS Gen2 (Bronze).\n",
    "\n",
    "This notebook is designed to run as a Databricks Job task called **Bronze** and pass paths/params to downstream tasks via `dbutils.jobs.taskValues`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f42d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks widgets (ADF / Jobs can override these)\n",
    "dbutils.widgets.text(\"storage_account\", \"rtyjkjhjytre456y\")\n",
    "dbutils.widgets.text(\"bronze_container\", \"bronze\")\n",
    "dbutils.widgets.text(\"silver_container\", \"silver\")\n",
    "dbutils.widgets.text(\"gold_container\", \"gold\")\n",
    "dbutils.widgets.text(\"start_date\", \"\")  # YYYY-MM-DD (optional)\n",
    "dbutils.widgets.text(\"end_date\", \"\")  # YYYY-MM-DD (optional)\n",
    "dbutils.widgets.text(\"lookback_days\", \"1\")\n",
    "dbutils.widgets.text(\"min_magnitude\", \"\")  # optional\n",
    "\n",
    "storage_account = dbutils.widgets.get(\"storage_account\").strip()\n",
    "bronze_container = dbutils.widgets.get(\"bronze_container\").strip()\n",
    "silver_container = dbutils.widgets.get(\"silver_container\").strip()\n",
    "gold_container = dbutils.widgets.get(\"gold_container\").strip()\n",
    "start_date_str = dbutils.widgets.get(\"start_date\").strip()\n",
    "end_date_str = dbutils.widgets.get(\"end_date\").strip()\n",
    "lookback_days = int(dbutils.widgets.get(\"lookback_days\").strip() or \"1\")\n",
    "min_magnitude_str = dbutils.widgets.get(\"min_magnitude\").strip()\n",
    "\n",
    "assert storage_account, \"storage_account is required\"\n",
    "\n",
    "bronze_adls = f\"abfss://{bronze_container}@{storage_account}.dfs.core.windows.net/\"\n",
    "silver_adls = f\"abfss://{silver_container}@{storage_account}.dfs.core.windows.net/\"\n",
    "gold_adls = f\"abfss://{gold_container}@{storage_account}.dfs.core.windows.net/\"\n",
    "\n",
    "# Optional: quick access check (won't fail the run if listing is blocked)\n",
    "try:\n",
    "    _ = dbutils.fs.ls(bronze_adls)\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] Could not list {bronze_adls}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0db86bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "\n",
    "def _parse_yyyy_mm_dd(s: str):\n",
    "    return datetime.strptime(s, \"%Y-%m-%d\").date()\n",
    "\n",
    "\n",
    "# Date range\n",
    "if start_date_str:\n",
    "    start_date = _parse_yyyy_mm_dd(start_date_str)\n",
    "else:\n",
    "    start_date = date.today() - timedelta(days=lookback_days)\n",
    "\n",
    "if end_date_str:\n",
    "    end_date = _parse_yyyy_mm_dd(end_date_str)\n",
    "else:\n",
    "    end_date = date.today()\n",
    "\n",
    "if end_date < start_date:\n",
    "    raise ValueError(f\"end_date ({end_date}) must be >= start_date ({start_date})\")\n",
    "\n",
    "run_date = start_date.isoformat()  # used as the partition folder in ADLS\n",
    "print(f\"Pulling USGS earthquakes from {start_date} to {end_date} (run_date={run_date})\")\n",
    "\n",
    "# Requests session w/ retries\n",
    "session = requests.Session()\n",
    "retries = Retry(\n",
    "    total=5,\n",
    "    connect=5,\n",
    "    read=5,\n",
    "    backoff_factor=0.5,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\"],\n",
    ")\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "USGS_URL = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "params = {\n",
    "    \"format\": \"geojson\",\n",
    "    \"starttime\": start_date.isoformat(),\n",
    "    \"endtime\": end_date.isoformat(),\n",
    "}\n",
    "if min_magnitude_str:\n",
    "    params[\"minmagnitude\"] = float(min_magnitude_str)\n",
    "\n",
    "resp = session.get(USGS_URL, params=params, timeout=60)\n",
    "resp.raise_for_status()\n",
    "geojson = resp.json()\n",
    "features = geojson.get(\"features\", [])\n",
    "print(f\"USGS returned {len(features)} events\")\n",
    "\n",
    "if not features:\n",
    "    # Don't hard-fail. Just pass empty outputs downstream so Silver/Gold can no-op if desired.\n",
    "    print(\"[INFO] No events returned for the date range.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ac107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write raw data to Bronze\n",
    "# NOTE: We write TWO things:\n",
    "#  1) Full payload (single JSON blob) for reprocessing/audit\n",
    "#  2) Features-only (JSON objects) for easy Spark reads\n",
    "\n",
    "bronze_payload_path = f\"{bronze_adls}earthquake_payload/run_date={run_date}/\"\n",
    "bronze_features_path = f\"{bronze_adls}earthquake_features/run_date={run_date}/\"\n",
    "legacy_features_path = f\"{bronze_adls}{run_date}_earthquake_data.json\"  # kept for backwards-compat\n",
    "\n",
    "# 1) Full payload as text (directory w/ a single part file)\n",
    "payload_rdd = spark.sparkContext.parallelize([json.dumps(geojson)])\n",
    "payload_rdd.coalesce(1).saveAsTextFile(bronze_payload_path)\n",
    "\n",
    "# 2) Features as JSON objects (directory)\n",
    "feature_json = [json.dumps(f) for f in features]\n",
    "features_rdd = spark.sparkContext.parallelize(feature_json)\n",
    "features_df = spark.read.json(features_rdd)\n",
    "features_df.write.mode(\"overwrite\").json(bronze_features_path)\n",
    "\n",
    "# Legacy single-path read location (still a directory, Spark can read it)\n",
    "features_df.write.mode(\"overwrite\").json(legacy_features_path)\n",
    "\n",
    "print(f\"Wrote payload  -> {bronze_payload_path}\")\n",
    "print(f\"Wrote features -> {bronze_features_path}\")\n",
    "print(f\"Wrote legacy   -> {legacy_features_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48eb15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass outputs to downstream tasks\n",
    "output_data = {\n",
    "    \"run_date\": run_date,\n",
    "    \"start_date\": start_date.isoformat(),\n",
    "    \"end_date\": end_date.isoformat(),\n",
    "    \"storage_account\": storage_account,\n",
    "    \"bronze_adls\": bronze_adls,\n",
    "    \"silver_adls\": silver_adls,\n",
    "    \"gold_adls\": gold_adls,\n",
    "    \"bronze_payload_path\": bronze_payload_path,\n",
    "    \"bronze_features_path\": bronze_features_path,\n",
    "    \"legacy_features_path\": legacy_features_path,\n",
    "    \"event_count\": int(len(features)),\n",
    "}\n",
    "dbutils.jobs.taskValues.set(key=\"bronze_output\", value=output_data)\n",
    "print(\"bronze_output:\", output_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
