{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Silver \u2014 Flatten + Clean\n",
        "\n",
        "Read Bronze JSON, normalize the nested GeoJSON into a tidy table, validate columns, and write Silver outputs to ADLS.\n",
        "\n",
        "Designed to run as a Databricks Job task called **Silver**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.types import TimestampType\n",
        "\n",
        "bronze_output = dbutils.jobs.taskValues.get(taskKey='Bronze', key='bronze_output')\n",
        "run_date = bronze_output.get('run_date')\n",
        "bronze_features_path = bronze_output.get('bronze_features_path') or bronze_output.get('legacy_features_path')\n",
        "silver_adls = bronze_output.get('silver_adls')\n",
        "\n",
        "assert run_date, 'Missing run_date from bronze_output'\n",
        "assert bronze_features_path, 'Missing bronze_features_path from bronze_output'\n",
        "assert silver_adls, 'Missing silver_adls from bronze_output'\n",
        "\n",
        "print(f'Reading Bronze features from: {bronze_features_path}')\n",
        "df_raw = spark.read.json(bronze_features_path)\n",
        "print('Raw schema:')\n",
        "df_raw.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Flatten GeoJSON feature into tabular rows\n",
        "df = (\n",
        "    df_raw\n",
        "    .select(\n",
        "        F.col('id').alias('event_id'),\n",
        "        F.col('geometry.coordinates').getItem(0).cast('double').alias('longitude'),\n",
        "        F.col('geometry.coordinates').getItem(1).cast('double').alias('latitude'),\n",
        "        F.col('geometry.coordinates').getItem(2).cast('double').alias('elevation_km'),\n",
        "        F.col('properties.title').alias('title'),\n",
        "        F.col('properties.place').alias('place_description'),\n",
        "        F.col('properties.sig').cast('int').alias('sig'),\n",
        "        F.col('properties.mag').cast('double').alias('mag'),\n",
        "        F.col('properties.magType').alias('mag_type'),\n",
        "        F.col('properties.time').cast('long').alias('time_ms'),\n",
        "        F.col('properties.updated').cast('long').alias('updated_ms'),\n",
        "    )\n",
        ")\n",
        "\n",
        "# Convert epoch-millis to timestamps\n",
        "df = (\n",
        "    df\n",
        "    .withColumn('event_ts', F.to_timestamp(F.from_unixtime(F.col('time_ms') / 1000)))\n",
        "    .withColumn('updated_ts', F.to_timestamp(F.from_unixtime(F.col('updated_ms') / 1000)))\n",
        "    .withColumn('event_date', F.to_date('event_ts'))\n",
        "    .withColumn('run_date', F.lit(run_date))\n",
        "    .drop('time_ms', 'updated_ms')\n",
        ")\n",
        "\n",
        "# Basic validation + cleaning\n",
        "df = df.filter(F.col('event_id').isNotNull())\n",
        "df = df.filter((F.col('latitude').between(-90, 90)) & (F.col('longitude').between(-180, 180)))\n",
        "df = df.filter(F.col('event_ts').isNotNull())\n",
        "\n",
        "# Dedupe by event_id, keep the most recently updated\n",
        "w = Window.partitionBy('event_id').orderBy(F.col('updated_ts').desc_nulls_last())\n",
        "df = df.withColumn('_rn', F.row_number().over(w)).filter(F.col('_rn') == 1).drop('_rn')\n",
        "\n",
        "print('Silver preview:')\n",
        "display(df.limit(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write Silver output (idempotent by run_date)\n",
        "silver_output_path = f\"{silver_adls}earthquake_events_silver/run_date={run_date}/\"\n",
        "\n",
        "(\n",
        "  df\n",
        "  .repartition('event_date')\n",
        "  .write\n",
        "  .mode('overwrite')\n",
        "  .parquet(silver_output_path)\n",
        ")\n",
        "\n",
        "row_count = df.count()\n",
        "print(f'Wrote {row_count} rows -> {silver_output_path}')\n",
        "\n",
        "dbutils.jobs.taskValues.set(key='silver_output', value={\n",
        "    'run_date': run_date,\n",
        "    'silver_output_path': silver_output_path,\n",
        "    'silver_row_count': int(row_count),\n",
        "})\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}