{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gold \u2014 Enrich + Aggregate\n",
        "\n",
        "Enrich Silver events with a country code (offline reverse geocode), classify significance, and write Gold outputs.\n",
        "\n",
        "Outputs:\n",
        "- **Event-level** enriched parquet\n",
        "- **Daily KPI rollups** (by date/country/sig_class) as CSV\n",
        "\n",
        "Designed to run as a Databricks Job task called **Gold**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "import pandas as pd\n",
        "\n",
        "bronze_output = dbutils.jobs.taskValues.get(taskKey='Bronze', key='bronze_output')\n",
        "silver_output = dbutils.jobs.taskValues.get(taskKey='Silver', key='silver_output')\n",
        "\n",
        "run_date = bronze_output.get('run_date')\n",
        "gold_adls = bronze_output.get('gold_adls')\n",
        "silver_output_path = silver_output.get('silver_output_path') if isinstance(silver_output, dict) else silver_output\n",
        "\n",
        "assert run_date, 'Missing run_date from bronze_output'\n",
        "assert gold_adls, 'Missing gold_adls from bronze_output'\n",
        "assert silver_output_path, 'Missing silver_output_path from silver_output'\n",
        "\n",
        "print(f'Reading Silver from: {silver_output_path}')\n",
        "df = spark.read.parquet(silver_output_path)\n",
        "print(f'Silver rows: {df.count()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Offline reverse geocoding (requires cluster library: reverse_geocoder)\n",
        "try:\n",
        "    import reverse_geocoder as rg\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\n",
        "        'Missing dependency: reverse_geocoder. Install it on the cluster (pip install reverse_geocoder) and re-run.'\n",
        "    ) from e\n",
        "\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "\n",
        "@pandas_udf('string')\n",
        "def country_code(lat: pd.Series, lon: pd.Series) -> pd.Series:\n",
        "    # Vectorized country code lookup using reverse_geocoder.search(list_of_coords)\n",
        "    mask = lat.notna() & lon.notna()\n",
        "    out = pd.Series([None] * len(lat))\n",
        "    if mask.any():\n",
        "        coords = list(zip(lat[mask].astype(float), lon[mask].astype(float)))\n",
        "        results = rg.search(coords)\n",
        "        codes = [r.get('cc') if r else None for r in results]\n",
        "        out.loc[mask] = codes\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Enrich + classify\n",
        "df_enriched = (\n",
        "    df\n",
        "    .withColumn('country_code', country_code(F.col('latitude'), F.col('longitude')))\n",
        "    .withColumn(\n",
        "        'sig_class',\n",
        "        F.when(F.col('sig') < 100, F.lit('Low'))\n",
        "         .when((F.col('sig') >= 100) & (F.col('sig') < 500), F.lit('Moderate'))\n",
        "         .otherwise(F.lit('High'))\n",
        "    )\n",
        ")\n",
        "\n",
        "display(df_enriched.limit(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write Gold event-level parquet (idempotent by run_date)\n",
        "gold_events_path = f\"{gold_adls}earthquake_events_gold/run_date={run_date}/\"\n",
        "(\n",
        "  df_enriched\n",
        "  .repartition('event_date')\n",
        "  .write\n",
        "  .mode('overwrite')\n",
        "  .parquet(gold_events_path)\n",
        ")\n",
        "\n",
        "event_count = df_enriched.count()\n",
        "print(f'Wrote enriched events ({event_count}) -> {gold_events_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# KPI rollups (daily x country x sig_class)\n",
        "kpis = (\n",
        "    df_enriched\n",
        "    .groupBy('event_date', 'country_code', 'sig_class')\n",
        "    .agg(\n",
        "        F.count('*').alias('event_count'),\n",
        "        F.avg('mag').alias('avg_mag'),\n",
        "        F.max('mag').alias('max_mag'),\n",
        "        F.expr('percentile_approx(mag, 0.95)').alias('p95_mag'),\n",
        "    )\n",
        ")\n",
        "\n",
        "gold_kpis_path = f\"{gold_adls}earthquake_kpis_gold/run_date={run_date}/\"\n",
        "(\n",
        "  kpis\n",
        "  .coalesce(1)\n",
        "  .write\n",
        "  .mode('overwrite')\n",
        "  .option('header', 'true')\n",
        "  .csv(gold_kpis_path)\n",
        ")\n",
        "\n",
        "print(f'Wrote KPIs -> {gold_kpis_path}')\n",
        "display(kpis.orderBy(F.desc('event_count')).limit(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dbutils.jobs.taskValues.set(key='gold_output', value={\n",
        "    'run_date': run_date,\n",
        "    'gold_events_path': gold_events_path,\n",
        "    'gold_kpis_path': gold_kpis_path,\n",
        "    'gold_event_count': int(event_count),\n",
        "})\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}